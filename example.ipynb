{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data import Dictionary, Corpus, PAD_INDEX\n",
    "from mst import mst\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(S_arc, heads):\n",
    "    fig, ax = plt.subplots()\n",
    "    # Make a 0/1 gold adjacency matrix.\n",
    "    n = heads.size(1)\n",
    "    G = np.zeros((n, n))\n",
    "    heads = heads.squeeze().data.numpy()\n",
    "    G[heads, np.arange(n)] = 1.\n",
    "    im = ax.imshow(G, vmin=0, vmax=1)\n",
    "    fig.colorbar(im)\n",
    "    plt.savefig('img/gold.pdf')\n",
    "    plt.cla()\n",
    "    # Plot the predicted adjacency matrix\n",
    "    A = F.softmax(S_arc.squeeze(0), dim=0)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(A.data.numpy(), vmin=0, vmax=1)\n",
    "    fig.colorbar(im)\n",
    "    plt.savefig('img/a.pdf')\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def predict(model, words, tags):\n",
    "    assert type(words) == type(tags)\n",
    "    if type(words) == type(tags) == list:\n",
    "        # Convert the lists into input for the PyTorch model.\n",
    "        words = Variable(torch.LongTensor([words]))\n",
    "        tags = Variable(torch.LongTensor([tags]))\n",
    "    # Dissable dropout.\n",
    "    model.eval()\n",
    "    # Predict arc and label score matrices.\n",
    "    S_arc, S_lab = model(words, tags)\n",
    "\n",
    "    # Predict heads\n",
    "    S = S_arc[0].data.numpy()\n",
    "    heads = mst(S)\n",
    "\n",
    "    # Predict labels\n",
    "    S_lab = S_lab[0]\n",
    "    select = torch.LongTensor(heads).unsqueeze(0).expand(S_lab.size(0), -1)\n",
    "    select = Variable(select)\n",
    "    selected = torch.gather(S_lab, 1, select.unsqueeze(1)).squeeze(1)\n",
    "    _, labels = selected.max(dim=0)\n",
    "    labels = labels.data.numpy()\n",
    "    return heads, labels\n",
    "\n",
    "\n",
    "def predict_batch(S_arc, S_lab, tags):\n",
    "    # Predict heads\n",
    "    S = S_arc.data.numpy()\n",
    "    heads = mst(S)\n",
    "\n",
    "    # Predict labels\n",
    "    select = torch.LongTensor(heads).unsqueeze(0).expand(S_lab.size(0), -1)\n",
    "    select = Variable(select)\n",
    "    selected = torch.gather(S_lab, 1, select.unsqueeze(1)).squeeze(1)\n",
    "    _, labels = selected.max(dim=0)\n",
    "    labels = labels.data.numpy()\n",
    "    return heads, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiAffineParser(\n",
      "  (embedding): WordEmbedding(\n",
      "    (embedding): Embedding(19343, 300, padding_idx=0)\n",
      "    (dropout): Dropout(p=0.3)\n",
      "  )\n",
      "  (encoder): RecurrentEncoder(\n",
      "    (rnn): LSTM(300, 400, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  )\n",
      "  (arc_mlp_h): MLP(\n",
      "    (layers): Sequential(\n",
      "      (fc_0): Linear(in_features=800, out_features=500, bias=True)\n",
      "      (ReLU_0): ReLU()\n",
      "      (dropout_0): Dropout(p=0.3)\n",
      "      (fc_1): Linear(in_features=500, out_features=500, bias=True)\n",
      "      (ReLU_1): ReLU()\n",
      "      (dropout_1): Dropout(p=0.3)\n",
      "    )\n",
      "  )\n",
      "  (arc_mlp_d): MLP(\n",
      "    (layers): Sequential(\n",
      "      (fc_0): Linear(in_features=800, out_features=500, bias=True)\n",
      "      (ReLU_0): ReLU()\n",
      "      (dropout_0): Dropout(p=0.3)\n",
      "      (fc_1): Linear(in_features=500, out_features=500, bias=True)\n",
      "      (ReLU_1): ReLU()\n",
      "      (dropout_1): Dropout(p=0.3)\n",
      "    )\n",
      "  )\n",
      "  (lab_mlp_h): MLP(\n",
      "    (layers): Sequential(\n",
      "      (fc_0): Linear(in_features=800, out_features=100, bias=True)\n",
      "      (ReLU_0): ReLU()\n",
      "      (dropout_0): Dropout(p=0.3)\n",
      "      (fc_1): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (ReLU_1): ReLU()\n",
      "      (dropout_1): Dropout(p=0.3)\n",
      "    )\n",
      "  )\n",
      "  (lab_mlp_d): MLP(\n",
      "    (layers): Sequential(\n",
      "      (fc_0): Linear(in_features=800, out_features=100, bias=True)\n",
      "      (ReLU_0): ReLU()\n",
      "      (dropout_0): Dropout(p=0.3)\n",
      "      (fc_1): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (ReLU_1): ReLU()\n",
      "      (dropout_1): Dropout(p=0.3)\n",
      "    )\n",
      "  )\n",
      "  (arc_biaffine): BiAffine()\n",
      "  (lab_biaffine): BiAffine()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4b6c991487a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mS_arc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS_lab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS_arc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Project NLP\\biaffine-dependency-parser\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;34m\"\"\"Compute the score matrices for the arcs and labels.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'rnn'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mPAD_INDEX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# sentence_lenghts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'words'"
     ]
    }
   ],
   "source": [
    "data_path = 'data/ud/UD_English-EWT'\n",
    "vocab_path = 'vocab/train'\n",
    "model_path = 'checkpoints/enmodel.pt'\n",
    "\n",
    "dictionary = Dictionary(vocab_path)\n",
    "corpus = Corpus(data_path=data_path, vocab_path=vocab_path)\n",
    "model = torch.load(model_path)\n",
    "batches = corpus.train.batches(1)\n",
    "\n",
    "print(model)\n",
    "words, tags, heads, labels = next(batches)\n",
    "S_arc, S_lab = model(words, tags)\n",
    "\n",
    "plot(S_arc, heads)\n",
    "words = tags = [1, 2, 3, 4]\n",
    "heads_pred, labels_pred = predict(model, words, tags)\n",
    "print(heads_pred, '\\n', heads[0].data.numpy())\n",
    "print(labels_pred, '\\n', labels[0].data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
